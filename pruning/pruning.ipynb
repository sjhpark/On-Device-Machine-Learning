{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from indiv_utils import load_yaml, size_on_disk, get_layers, measure_inference_latency, param_count, FLOPs_count, save_model_weights, start_train, Sparsity\n",
    "from models import FFNN\n",
    "from data_processing import MNISTDataProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries & Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| hyperparameter  | MNIST |\n",
    "| --------------- | ----- |\n",
    "| learning rate   | 0.001 |\n",
    "| batch size      | 64    |\n",
    "| hidden size     | 1024  |\n",
    "| # hidden layers | 2     |\n",
    "| input size      | 20x20 |\n",
    "| output size     | 10    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Setup\"\"\"\n",
    "# hyperparameters\n",
    "lr = 0.001\n",
    "batch_size = 64\n",
    "num_hidden = 2\n",
    "hidden_dim = 1024\n",
    "out_dim = 10 # 10 MNIST classes   \n",
    "epochs = 2\n",
    "input_dim = 20*20\n",
    "\n",
    "# config\n",
    "config = load_yaml('config')\n",
    "\n",
    "# device \n",
    "device = torch.device(config['device'])\n",
    "\n",
    "# criterion\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# model\n",
    "model = FFNN(input_dim=input_dim, hidden_dim=hidden_dim, out_dim=out_dim, num_hidden=num_hidden, bias=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Model initialization:\n",
    "Before training, SAVE the model's initial (random) weights. \n",
    "You will use them later for iterative pruning.\"\"\"\n",
    "\n",
    "if os.path.exists(\"out/FFNN_weights_initial.pth\"):\n",
    "    pass\n",
    "else:\n",
    "    initial_random_weights = save_model_weights(model, fname=\"initial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train\"\"\"\n",
    "if os.path.exists(\"out/FFNN_weights_trained.pth\"):\n",
    "    pass\n",
    "else:\n",
    "    start_train(model, device, criterion, epochs, batch_size, lr)\n",
    "    trained_weights = save_model_weights(model, fname=\"trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded initial model weights\n",
      "Loaded trained model weights\n",
      "Center Cropping images from 28x28 to 20x20\n",
      "new image size:  (400,)\n",
      "Center Cropping images from 28x28 to 20x20\n",
      "new image size:  (400,)\n",
      "parsing test features...\n",
      "The number of test labels: 10000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"load initial model\"\"\"\n",
    "model_initial = FFNN(input_dim=input_dim, hidden_dim=hidden_dim, out_dim=out_dim, num_hidden=num_hidden, bias=True).to(device)\n",
    "model_initial.load_state_dict(torch.load(\"out/FFNN_weights_initial.pth\"))\n",
    "print(\"Loaded initial model weights\")\n",
    "\n",
    "\"\"\"load trained model\"\"\"\n",
    "model_trained = copy.deepcopy(model_initial)\n",
    "model_trained.load_state_dict(torch.load(\"out/FFNN_weights_trained.pth\"))\n",
    "print(\"Loaded trained model weights\")\n",
    "\n",
    "\"\"\"test dataset\"\"\"\n",
    "test_dataset = MNISTDataProcessor().vision_test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring inference latency of trained FFNN on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm-up begins...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:03, 3204.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean inference latency: 0.129ms\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Inference Latency of Trained Model\"\"\"\n",
    "# Inference Latency\n",
    "measure_inference_latency(model=model, test_dataset=test_dataset, device=device, warmup_itr=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters in each layer of FFNN:\n",
      "\tLinear: 400 * 1024 + 1024 = 410,624\n",
      "\tReLU: = 0\n",
      "\tLinear: 1024 * 1024 + 1024 = 1,049,600\n",
      "\tReLU: = 0\n",
      "\tLinear: 1024 * 10 + 10 = 10,250\n",
      "The total number of parameters in FFNN: 1,470,474\n",
      "The total FLOPs in FFNN: 0.002937 GFLOPs\n",
      "Model Size on Disk: 5.883903 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5883903"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Parameter Count, FLOPs, and Disk Storage of Trained Model\"\"\"\n",
    "# layers\n",
    "layers = get_layers(model=model)\n",
    "\n",
    "# Parameter Count\n",
    "param_count(model=model, layers=layers)\n",
    "\n",
    "# FLOPs\n",
    "FLOPs_count(model=model, layers=layers)\n",
    "\n",
    "# Disk Storage\n",
    "size_on_disk(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magnitude pruning on SST2/MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNN(\n",
      "  (model): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=400, out_features=1024, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (2): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Trained Model's architecture\"\"\"\n",
    "print(model_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Buffers of Trained Model\"\"\"\n",
    "print(list(model_trained.named_buffers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNN layers:\n",
      "[Linear(in_features=400, out_features=1024, bias=True), ReLU(), Linear(in_features=1024, out_features=1024, bias=True), ReLU(), Linear(in_features=1024, out_features=10, bias=True)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Get all layers of Trained Model\"\"\"\n",
    "model_layers = get_layers(model=model_trained)\n",
    "print(f\"{model_trained.__class__.__name__} layers:\\n{model_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight of the first layer:\n",
      "Parameter containing:\n",
      "tensor([[ 0.0661,  0.0115,  0.0095,  ...,  0.0792,  0.0295,  0.0438],\n",
      "        [ 0.0417,  0.0243,  0.0168,  ...,  0.0155,  0.0438,  0.0155],\n",
      "        [ 0.0864,  0.0559,  0.0999,  ...,  0.0503,  0.0688,  0.0363],\n",
      "        ...,\n",
      "        [-0.0159,  0.0330, -0.0122,  ...,  0.0150,  0.0357, -0.0115],\n",
      "        [-0.0207,  0.0613, -0.0179,  ..., -0.0278,  0.0120, -0.0376],\n",
      "        [ 0.0525,  0.0021,  0.0082,  ..., -0.0028,  0.0136,  0.0405]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Bias of the first layer:\n",
      "Parameter containing:\n",
      "tensor([ 0.0228, -0.0110, -0.0333,  ..., -0.0355, -0.0043, -0.0440],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Weight and Bias of the first layer of Trained Model\"\"\"\n",
    "print(f\"Weight of the first layer:\\n{model_layers[0].weight}\")\n",
    "print(f\"Bias of the first layer:\\n{model_layers[0].bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Unstructured Magnitude (L1) Pruning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global sparsity: 0.0%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Sparsity of Trained Model\"\"\"\n",
    "Sparsity(model_layers=model_layers).global_level()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLinear: 0.0%\n",
      "\tLinear: 0.0%\n",
      "\tLinear: 0.0%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Sparsity of each layer of Trained Model\"\"\"\n",
    "Sparsity(model_layers=model_layers).each_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Global Unstructured Pruning\"\"\"\n",
    "sparsity_level = 0.3\n",
    "example_prune_params = [(model_layers[0], 'weight'),\n",
    "                           (model_layers[2], 'weight'),\n",
    "                           (model_layers[4], 'weight')]\n",
    "\n",
    "prune.global_unstructured(example_prune_params, pruning_method=prune.L1Unstructured, amount=sparsity_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global sparsity: 30.00001362011855%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Sparsity of pruned Trained Model\"\"\"\n",
    "Sparsity(model_layers=model_layers).global_level()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLinear: 23.748291015625%\n",
      "\tLinear: 32.4275016784668%\n",
      "\tLinear: 31.494140625%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Sparsity of each layer of pruned Trained Model\"\"\"\n",
    "Sparsity(model_layers=model_layers).each_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative magnitude pruning (IMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('11767')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3bfde7030847683facd11df8195a3943390ddff51b50020d9e445892926ca1a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
